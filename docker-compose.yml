version: '3.9'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  spark-master:
    build: .
    container_name: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    ports:
      - "7077:7077"
      - "18080:8080"
    environment:
      SPARK_MASTER_HOST: spark-master
      SPARK_HOME: /opt/spark
    volumes:
      - ./app:/opt/spark/app
      - ./data:/opt/spark/data

  spark-worker:
    image: apache/spark:3.5.3
    container_name: spark-worker
    depends_on:
      - spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    ports:
      - "8081:8081"
    environment:
      SPARK_HOME: /opt/spark
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1G
      SPARK_DRIVER_MEMORY: 1G
      SPARK_EXECUTOR_MEMORY: 1G
    volumes:
      - ./app:/opt/spark/app
      - ./data:/opt/spark/data

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: 'Os51t=Ag/3=B'
      MYSQL_DATABASE: emergentETL
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql:/docker-entrypoint-initdb.d
    command: --default-authentication-plugin=mysql_native_password

  mongodb:
    image: mongo:6.0
    container_name: mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_DATABASE: gamc_db
    volumes:
      - mongo_data:/data/db

  # Consumer para EM310 Soterrados
  spark-consumer-em310:
    build: .
    container_name: spark-consumer-em310
    depends_on:
      - kafka
      - mysql
      - spark-master
    command: ["/opt/spark/bin/spark-submit", "--master", "local[2]", "/opt/spark/app/etl/spark_consumer_em310.py"]
    volumes:
      - ./app:/opt/spark/app
      - ./data:/opt/spark/data
    environment:
      SPARK_HOME: /opt/spark

  # Consumer para EM500 CO2
  spark-consumer-em500:
    build: .
    container_name: spark-consumer-em500
    depends_on:
      - kafka
      - mysql
      - spark-master
    command: ["/opt/spark/bin/spark-submit", "--master", "local[2]", "/opt/spark/app/etl/spark_consumer_em500.py"]
    volumes:
      - ./app:/opt/spark/app
      - ./data:/opt/spark/data
    environment:
      SPARK_HOME: /opt/spark

  # Consumer para WS302 Sonido
  spark-consumer-ws302:
    build: .
    container_name: spark-consumer-ws302
    depends_on:
      - kafka
      - mysql
      - spark-master
    command: ["/opt/spark/bin/spark-submit", "--master", "local[2]", "/opt/spark/app/etl/spark_consumer_ws302.py"]
    volumes:
      - ./app:/opt/spark/app
      - ./data:/opt/spark/data
    environment:
      SPARK_HOME: /opt/spark

volumes:
  mongo_data:
  mysql_data:
